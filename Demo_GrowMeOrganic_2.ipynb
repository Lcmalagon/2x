{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28db1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv\n",
    "import time\n",
    "from time import sleep\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from parsel import Selector\n",
    "\n",
    "\n",
    "#Imports for automation without proxycurl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#To find additional xpaths\n",
    "#https://www.linkedin.com/pulse/techniques-find-xpath-selenium-webdriver-vivekanand-r-yadav/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df462b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part is getting the links using automation\n",
    "\n",
    "scrolling = False\n",
    "\n",
    "def scrolldown(driver, timeout = 20, sleep_interval = 5):\n",
    "    \"\"\"A method for scrolling the page within a timeout.\"\"\"\n",
    "    start_time = time.time()\n",
    "    scroll_count = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to the bottom.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        scroll_count += 1\n",
    "        # Wait to load the page.\n",
    "        time.sleep(sleep_interval)\n",
    "        # Calculate new scroll height and compare with last scroll height.\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"Reached end of page.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "        # Check if timeout is reached\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(\"Timeout reached.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Scroll count: {scroll_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc872806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping this\n",
    "\n",
    "# defining a function called login_linkedin\n",
    "\n",
    "def login_linkedin(file_name, driver, login_url = 'https://www.linkedin.com/login'):\n",
    "    # Open the file with context manager to ensure it's closed properly after reading\n",
    "    with open(file_name, 'r') as credential:\n",
    "        uname = credential.readline().strip()  # Read the first line for username\n",
    "        password = credential.readline().strip()  # Read the second line for password\n",
    "\n",
    "    print('- Finish importing the login credentials')\n",
    "    \n",
    "   \n",
    "    # Navigate to the LinkedIn login page\n",
    "    driver.get(login_url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the username field to become available and enter the username\n",
    "        username = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"username\"))\n",
    "        )\n",
    "        \n",
    "        time.sleep(2)\n",
    "        username.send_keys(uname)\n",
    "\n",
    "        # Wait for the password field to become available and enter the password\n",
    "        pword = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"password\"))\n",
    "        )\n",
    "        \n",
    "        time.sleep(2)\n",
    "        pword.send_keys(password)\n",
    "\n",
    "        # Wait for the submit button to become clickable and click it\n",
    "        login_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\"))\n",
    "        )\n",
    "        \n",
    "        time.sleep(2)\n",
    "        login_button.click()\n",
    "\n",
    "        print('Login successful')\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f'An error occurred during login: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7ed43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic driver function. \n",
    "\n",
    "def go_to_URL(driver, URL, scrolling = False, scroll_timeout = 0):\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "        # Wait for the necessary page elements to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        \n",
    "        # If scrolling is required, use the scrolldown function\n",
    "        if scrolling:\n",
    "            scrolldown(driver, timeout=scroll_timeout)\n",
    "            print(\"Alumni Block after scrolling\")\n",
    "    except WebDriverException as e:\n",
    "        print(f\"Failed to navigate to {URL} or an error occurred while scrolling: {e}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc95c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function and loads the csv file. Loads the csv file of links\n",
    "# https://medium.com/@udayshadani/errors-we-can-get-during-web-scraping-and-how-to-handle-it-41de0c2f6ec3\n",
    "\n",
    "def load_csv(file_name):\n",
    "    try:\n",
    "        return pd.read_csv(file_name)\n",
    "    except FileNotFoundError:\n",
    "        print('File not found error')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c61acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fields(field):\n",
    "    if field:\n",
    "        pass\n",
    "    else:\n",
    "        field = 'NO Results'\n",
    "    return field\n",
    "\n",
    "from lxml import html\n",
    "\n",
    "def extract_from_html(html_snippet):\n",
    "    # Parse the HTML content of the snippet\n",
    "    tree = html.fromstring(html_snippet)\n",
    "    \n",
    "    # Use XPath to select the text within the <h1> tag\n",
    "    name = tree.xpath(\"//h1/text()\")\n",
    "    \n",
    "    return name[0].strip() if name else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24929f8-3401-4ece-a006-e555c476184b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new updated function\n",
    "#3672 seba bus_dev\n",
    "#5587 consulting\n",
    "\n",
    "def automation_links(driver, csv_data, numberlinks = 2, start = 6948):\n",
    "    Jobdata = []\n",
    "    experience = []\n",
    "# its going to start and stop at this number\n",
    "    for i in csv_data['url'][start:start + numberlinks]:\n",
    "        #driver.get('https://www.linkedin.com/in/romaxavierm/')  # To get the profile URL\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        \n",
    "            \n",
    "        # Get education\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')  # Using BeautifulSoup to parse the HTML\n",
    "\n",
    "        # Find all <section> elements\n",
    "        section_list = soup\n",
    "        #soup.find_all(\"section\")\n",
    "\n",
    "        # Define a function to safely extract text using BeautifulSoup\n",
    "        def extract_text_by_xpath(soup, xpath):\n",
    "            elements = soup.select(xpath)\n",
    "            if elements:\n",
    "                return elements[0].get_text(strip=True)\n",
    "            return 'None'\n",
    "        # First, locate the experience section anchor\n",
    "        expAnchor = soup.find('div', id ='experience')\n",
    "        j = soup.select('section:has(#experience)>div>ul>li') # company element\n",
    "        company_list=[]\n",
    "        title_list=[]\n",
    "        extra_list=[]\n",
    "        for x in j:\n",
    "            #mr1.t-bold span[aria-hidden=\"true\"]\n",
    "            title = x.select_one('div[class=\"display-flex align-items-center mr1 t-bold\"] span') # title element\n",
    "            if title == None:\n",
    "                title =''\n",
    "                extra=''\n",
    "            else:\n",
    "                title=title.text\n",
    "                temp= x.select('span[class=\"t-14 t-normal t-black--light\"] span')\n",
    "                extra = [tt.text for tt in temp]\n",
    "            jj = x.select('span[class=\"t-14 t-normal\"] span')\n",
    "            jjj = [jj_item.text for jj_item in jj]\n",
    "            company_list.append(jjj[0])\n",
    "            title_list.append(title)\n",
    "            extra_list.append(extra[::2]) \n",
    "            \n",
    "        next_siblings = expAnchor.find_next_siblings()\n",
    "\n",
    "        # Concatenate the HTML of all these siblings\n",
    "        siblings_html = ''.join(str(sibling) for sibling in next_siblings)\n",
    "\n",
    "        # Parse this concatenated HTML into a new BeautifulSoup object\n",
    "        experience_section = BeautifulSoup(siblings_html, 'html.parser')\n",
    "\n",
    "        \n",
    "        # Define the Cascading Style Sheets selectors corresponding to the XPaths provided\n",
    "        name_selector = \"a.pv-text-details__about-this-profile-entrypoint h1\"\n",
    "        #expAnchor = soup.select('div.pvs-list__outer-container')\n",
    "        #expAnchor = soup.select('div#experience div.pvs-list__outer-container')\n",
    "            \n",
    "        company_selector = 'div.pvs-entity--padded span.t-14.t-normal span[aria-hidden=\"true\"]'\n",
    "        job_title_selector = 'div.display-flex.align-items-center.mr1.t-bold span[aria-hidden=\"true\"]'\n",
    "        university_name_selector = \"section.artdeco-card h2 span[aria-hidden='true']:contains('Education') ~ div.display-flex align-items-center mr1 hoverable-link-text t-bold span[aria-hidden='true']\"\n",
    "        degree_selector = \"section.artdeco-card h2 span[aria-hidden='true']:contains('Education') ~ div.pvs-list__outer-container li a.optional-action-target-wrapper.display-flex.flex-column.full-width span.t-14 span[aria-hidden='true']\"\n",
    "        dates_selector = \"section.artdeco-card h2 span[aria-hidden='true']:contains('Education') ~ div.pvs-list__outer-container li a.optional-action-target-wrapper.display-flex.flex-column.full-width span.t-black--light span[aria-hidden='true']\"\n",
    "            \n",
    "        #location_selector = \"div.pv-text-details__left-panel.mt2 span.text-body-small.inline.t-black--light.break-words\"\n",
    "        #email_selector = \"div.pv-contact-info__ci-container a[href^='mailto:']\"\n",
    "            \n",
    "        # Now you can use CSS selectors on experience_section\n",
    "        company_elements = experience_section.select(company_selector)\n",
    "        job_title_elements = experience_section.select(job_title_selector)\n",
    "\n",
    "\n",
    "\n",
    "        # Extract data using BeautifulSoup selectors\n",
    "        name = extract_text_by_xpath(soup, name_selector)\n",
    "        companies = [company.get_text().strip() for company in company_elements]\n",
    "        job_titles = [title.get_text().strip() for title in job_title_elements]\n",
    "        #location = extract_text_by_xpath(soup, location_selector)\n",
    "        #email = extract_text_by_xpath(soup, email_selector)\n",
    "\n",
    "        # Validate fields if necessary (assuming you have a validate_fields function)\n",
    "        name = validate_fields(name)\n",
    "        companies = validate_fields(companies)\n",
    "        job_titles = validate_fields(job_titles)\n",
    "        #location = validate_fields(location)\n",
    "        #email = validate_fields(email)\n",
    "            \n",
    "        # Assuming 'soup' is your BeautifulSoup object containing the parsed HTML\n",
    "        colleges = []\n",
    "        degrees = []  # List to store degree names\n",
    "        dates_list = []  # List to store dates\n",
    "\n",
    "        # Selectors targeting a more specific range within the education section\n",
    "        college_elements = soup.select('section.artdeco-card li a[href*=\"linkedin.com/company\"] div.display-flex.align-items-center.mr1.hoverable-link-text.t-bold span[aria-hidden=\"true\"]')\n",
    "        degree_elements = soup.select('section.artdeco-card li a[href*=\"linkedin.com/company\"] span.t-14 span[aria-hidden=\"true\"]')\n",
    "        date_elements = soup.select('section.artdeco-card li a[href*=\"linkedin.com/company\"] span.t-black--light span[aria-hidden=\"true\"]')\n",
    "\n",
    "        # Regular expression to match strings containing digits\n",
    "        digit_pattern = re.compile(r'\\d')\n",
    "            \n",
    "                \n",
    "        for college in college_elements:\n",
    "            text = college.get_text(strip=True)\n",
    "            # Check if the text contains \"university\" or \"college\" (case-insensitive)\n",
    "            if \"university\" in text.lower() or \"college\" in text.lower():\n",
    "                colleges.append(text)\n",
    "\n",
    "        for degree in degree_elements:\n",
    "            text = degree.get_text(strip=True).lower()  # Convert to lower case for case-insensitive comparison\n",
    "            if (not digit_pattern.search(text) \n",
    "                and 'followers' not in text\n",
    "                and 'ca' not in text\n",
    "                and 'california' not in text\n",
    "                and 'time' not in text):\n",
    "                degrees.append(text.title())  # Convert back to title case for display\n",
    "\n",
    "        for date in date_elements:\n",
    "            text = date.get_text(strip=True)\n",
    "            if digit_pattern.search(text) and 'followers' not in text.lower() and 'mos' not in text.lower():  # Include text with digits but exclude 'followers' and 'mos'\n",
    "                dates_list.append(text)\n",
    "                    \n",
    "        # Filter out empty or \"Unknown\" job titles and companies\n",
    "        job_titles = [title for title in job_titles if title.strip() not in ('', 'Unknown')]\n",
    "        companies = [company for company in companies if company.strip() not in ('', 'Unknown')]\n",
    "\n",
    "        # Exclude entries that contain common keywords that are not job titles or companies\n",
    "        exclude_keywords = ['followers', 'education', 'members', 'published', '·']\n",
    "        job_titles = [title for title in job_titles if all(keyword not in title.lower() for keyword in exclude_keywords)]\n",
    "        companies = [company for company in companies if all(keyword not in company.lower() for keyword in exclude_keywords)]\n",
    "\n",
    "        # Check the length of job titles and companies to exclude very short or suspicious entries\n",
    "        min_length = 2  # Minimum length for job titles and companies\n",
    "        job_titles = [title for title in job_titles if len(title.split()) >= min_length]\n",
    "        companies = [company for company in companies if len(company.split()) >= min_length]\n",
    "            \n",
    "\n",
    "\n",
    "        # Printing the outputs\n",
    "        print('\\n')\n",
    "        print('Name:', name)\n",
    "        print('Link:', i)\n",
    "        print('Experience: ', title_list)\n",
    "        print('Employers: ', company_list)\n",
    "        print('Colleges: ', colleges)\n",
    "        print('Degrees: ', degrees)\n",
    "        print('Dates Attended: ', dates_list)\n",
    "        print('Extra Info: ', extra_list)\n",
    "\n",
    "        # Storing data in a dictionary\n",
    "        data = {\n",
    "            'Name': name,\n",
    "            'url': i,\n",
    "            'Experience': title_list,\n",
    "            'Employers': company_list,\n",
    "            'Colleges': colleges,\n",
    "            'Degrees': degrees,\n",
    "            'Dates Attended': dates_list,\n",
    "            'Extra Info:': extra_list\n",
    "            }\n",
    "        Jobdata.append(data)\n",
    "        \n",
    "\n",
    "    # Exporting this into an Excel file\n",
    "    df = pd.DataFrame(Jobdata)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3a3893-e8e2-4166-b09f-a389f63d298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finish importing the login credentials\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# logs you into linkedin\n",
    "\n",
    "# Call all functions to go get links\n",
    "\n",
    "opts = Options()\n",
    "\n",
    "# using my useragent?\n",
    "\n",
    "opts.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n",
    "                                                               \n",
    "#opts.add_argument(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:53.0) Gecko/20100101 Firefox/53.0\")\n",
    "\n",
    "driver = webdriver.Chrome(options = opts)\n",
    "\n",
    "#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "Login_URL = r\"https://linkedin.com/uas/login\"\n",
    "driver.get(Login_URL)\n",
    "\n",
    "# waiting for page to load\n",
    "sleep(3)\n",
    "\n",
    "file = \"login_credential.txt\"\n",
    "\n",
    "login_linkedin(file, driver)\n",
    "\n",
    "Saint_Marys_URL = r\"https://www.linkedin.com/school/saint-mary's-college-of-california/people/\"\n",
    "\n",
    "go_to_URL(driver, Saint_Marys_URL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6f1a6ce-7137-4bf0-9ea3-15f870389133",
   "metadata": {
    "tags": []
   },
   "source": [
    "opts = Options()\n",
    "\n",
    "opts.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options = opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a5d60e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    url\n",
      "0     https://www.linkedin.com/in/ACwAAABvQKABkFKBAu...\n",
      "1     https://www.linkedin.com/in/ACwAAAER0qIBV5XZpx...\n",
      "2     https://www.linkedin.com/in/ACwAAATJ8wsBMoEv2e...\n",
      "3     https://www.linkedin.com/in/ACwAAAYgqzIBjNJ8iV...\n",
      "4     https://www.linkedin.com/in/ACwAAAg8-XoBNsL5in...\n",
      "...                                                 ...\n",
      "8345  https://www.linkedin.com/in/ACwAAANE6h0BacqAyI...\n",
      "8346  https://www.linkedin.com/in/ACwAAAa2Db0BmhhxJL...\n",
      "8347  https://www.linkedin.com/in/ACwAAB-n5GUBfwG9mz...\n",
      "8348  https://www.linkedin.com/in/ACwAAEBtS2UBkGyUwV...\n",
      "8349  https://www.linkedin.com/in/ACwAAEFd3dkB5saXfs...\n",
      "\n",
      "[8350 rows x 1 columns]\n",
      "\n",
      "\n",
      "Name: Jon (Jonathan) Gottfried\n",
      "Link: https://www.linkedin.com/in/ACwAAAAG9L8B1gDGO8-lNyM_WXd9fY4N85xAcLQ\n",
      "Experience:  ['CFO/COO', 'VP', 'Principal', 'Validation Manager', 'Validation Supervisor']\n",
      "Employers:  ['TekTeam · Full-time', 'AQS', 'GW Associates', 'Pharmtech', 'VSI']\n",
      "Colleges:  ['St Mary’s University, Twickenham', \"Saint Mary's College of California\"]\n",
      "Degrees:  ['Mba, Executive', 'Mba, Business']\n",
      "Dates Attended:  ['Jan 2001 - Jun 2002', '2000 - 2001']\n",
      "Extra Info:  [['Mar 1999 - Present · 24 yrs 10 mos', 'SF Peninsula'], ['Jan 1997 - Dec 1999 · 3 yrs'], ['Jan 1997 - Jan 1998 · 1 yr 1 mo'], ['Jul 1996 - Mar 1997 · 9 mos'], ['Jan 1996 - Jul 1996 · 7 mos']]\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.199)\nStacktrace:\n0   chromedriver                        0x0000000105fc4d28 chromedriver + 4795688\n1   chromedriver                        0x0000000105fbc2b3 chromedriver + 4760243\n2   chromedriver                        0x0000000105b9588d chromedriver + 407693\n3   chromedriver                        0x0000000105b69758 chromedriver + 227160\n4   chromedriver                        0x0000000105c1150f chromedriver + 914703\n5   chromedriver                        0x0000000105c270e6 chromedriver + 1003750\n6   chromedriver                        0x0000000105c0ba73 chromedriver + 891507\n7   chromedriver                        0x0000000105bd6143 chromedriver + 672067\n8   chromedriver                        0x0000000105bd731e chromedriver + 676638\n9   chromedriver                        0x0000000105f85795 chromedriver + 4536213\n10  chromedriver                        0x0000000105f8a853 chromedriver + 4556883\n11  chromedriver                        0x0000000105f6b001 chromedriver + 4427777\n12  chromedriver                        0x0000000105f8b59d chromedriver + 4560285\n13  chromedriver                        0x0000000105f5c48c chromedriver + 4367500\n14  chromedriver                        0x0000000105fab0e8 chromedriver + 4690152\n15  chromedriver                        0x0000000105fab29e chromedriver + 4690590\n16  chromedriver                        0x0000000105fbbeee chromedriver + 4759278\n17  libsystem_pthread.dylib             0x00007ff8152c7202 _pthread_start + 99\n18  libsystem_pthread.dylib             0x00007ff8152c2bab thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m csv_data \u001b[38;5;241m=\u001b[39m load_csv(csv_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(csv_data)\n\u001b[0;32m----> 9\u001b[0m df2 \u001b[38;5;241m=\u001b[39m automation_links(driver, csv_data)\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mautomation_links\u001b[0;34m(driver, csv_data, numberlinks, start)\u001b[0m\n\u001b[1;32m     12\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get education\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Using BeautifulSoup to parse the HTML\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Find all <section> elements\u001b[39;00m\n\u001b[1;32m     19\u001b[0m section_list \u001b[38;5;241m=\u001b[39m soup\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET_PAGE_SOURCE)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[1;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.199)\nStacktrace:\n0   chromedriver                        0x0000000105fc4d28 chromedriver + 4795688\n1   chromedriver                        0x0000000105fbc2b3 chromedriver + 4760243\n2   chromedriver                        0x0000000105b9588d chromedriver + 407693\n3   chromedriver                        0x0000000105b69758 chromedriver + 227160\n4   chromedriver                        0x0000000105c1150f chromedriver + 914703\n5   chromedriver                        0x0000000105c270e6 chromedriver + 1003750\n6   chromedriver                        0x0000000105c0ba73 chromedriver + 891507\n7   chromedriver                        0x0000000105bd6143 chromedriver + 672067\n8   chromedriver                        0x0000000105bd731e chromedriver + 676638\n9   chromedriver                        0x0000000105f85795 chromedriver + 4536213\n10  chromedriver                        0x0000000105f8a853 chromedriver + 4556883\n11  chromedriver                        0x0000000105f6b001 chromedriver + 4427777\n12  chromedriver                        0x0000000105f8b59d chromedriver + 4560285\n13  chromedriver                        0x0000000105f5c48c chromedriver + 4367500\n14  chromedriver                        0x0000000105fab0e8 chromedriver + 4690152\n15  chromedriver                        0x0000000105fab29e chromedriver + 4690590\n16  chromedriver                        0x0000000105fbbeee chromedriver + 4759278\n17  libsystem_pthread.dylib             0x00007ff8152c7202 _pthread_start + 99\n18  libsystem_pthread.dylib             0x00007ff8152c2bab thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "# setting up the links to use GrowMeOrganic Code to create the database.\n",
    "\n",
    "csv_name = 'links.csv'\n",
    "\n",
    "csv_data = load_csv(csv_name)\n",
    "\n",
    "print(csv_data)\n",
    "\n",
    "df2 = automation_links(driver, csv_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the database to the csv file.\n",
    "\n",
    "# Specify the filename\n",
    "\n",
    "print(df2)\n",
    "\n",
    "filename = 'new_36.csv'\n",
    "\n",
    "# Write the DataFrame to a .csv file\n",
    "df2.to_csv(filename, index = False, encoding ='utf-8')\n",
    "\n",
    "print(f\"Data has been written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c382ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
